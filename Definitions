import numpy as np
import matplotlib.pyplot as plt
import multiprocessing as mp

def convolution(image, filter):
    empty = np.zeros((image_size-2, image_size-2))
    dot_1 = 0
    for j in range(26):
     for i in range(26):
         for l in range(filter.shape[0]):
             for k in range(filter.shape[1]):
                 dot_1 = dot_1 + image[i+l, j+k]*filter[l, k]
                 if k==filter.shape[1]-1 and l==filter.shape[0]-1:
                     empty[i,j] = dot_1
    return(empty)

def maxpool(convol, pool):
    empty = np.zeros((24, 24))         
    for j in range(0, 24):                                  #stride of 2 horizontally
     for i in range(0, 24):                                 #stride of 2 vertically
         #dot_1 = 0
         for l in range(pool):
             for k in range(pool):
                 pooling = np.zeros((2,2))
                 pooling[l,k] = convol[i+l, j+k]
                 
         empty[i, j] = np.max(pooling)        
          
                 #if dot_1 < convol[i+l, j+k]:
                     #dot_1 =  convol[i+l, j+k]*filter[l, k]
                 #if k==pool.shape[1]-1 and l==pool.shape[0]-1:
                     #empty[i,j] = dot_1
    return(empty)

def ReLUactivation(input):
     for i,v in enumerate(input):
         if v<0:
             input[i]=0
     return(input)

def softmax(x):                                          #Use np.float64 for input
  e_x = np.exp(x) 
  return e_x / e_x.sum(axis=0)

def sigmoid(x):
    return 1/(1+np.exp(-x))

def meansquaredcost(prob, hotoneempty, train_labels, i):
 outputcost=0
 cost1 = 0
 for j in range(prob.shape[0]):
  outputcost = np.sum((prob-hotoneempty[j])**2)/(10) 
 return outputcost

  #hotone representation of labels
def hotone(train_labels):
 hotoneempty = np.zeros(shape = (train_labels.shape[0],10))
 for i in range(train_labels.shape[0]):
  lol = int(train_labels[i, 0])
  hotoneempty[i, lol] = 1
  
  return hotoneempty

def forward1(input, weights, biases):   #make sure input, weights and biases are all numpy arrays
 neuron_no = len(input)
 output = weights @ input
 output1 = output + biases
 return output1

def img_diag_flip(input):
    input1 = np.asarray(input)
    return input1.T

def img_hori_flip(img_training):
    input1 = np.zeros((img_training.shape[0], img_training.shape[1]))
    for i in range(img_training.shape[0]):
        for j in range(img_training.shape[1]):
            input1[-i-1, j] = img_training[i, j]
    return input1

def img_vert_flip(img_training):
    input1 = np.zeros((img_training.shape[0], img_training.shape[1]))
    for i in range(img_training.shape[0]):
        for j in range(img_training.shape[1]):
            input1[i, -j-1] = img_training[i, j]
    return input1


def sigmoid(x):
    return 1 / (1 + math.exp(-x))

def CrossEntropy(yHat, y):
    if y == 1:
      return -log(yHat)
    else:
      return -log(1 - yHat)   
  
def confusionmatrix(TP, TN, FP, FN):
      return np.asarray([[TP, FP], [FN, TN]])

def accuracy(confusionmatrix):
     accuracy1 = (confusionmatrix[1,1] + confusionmatrix[2,2])/((confusionmatrix[1,1])+confusionmatrix[1,2]+confusionmatrix[2,1]+confusionmatrix[2,2])
     return accuracy1

def precision(confusionmatrix):
    precision1 = confusionmatrix[1,1]/(confusionmatrix[1,1]+confusionmatrix[1,2])
    return precision1

def recall(confusionmatrix):
    recall1 = confusionmatrix[1,1]/(confusionmatrix[1,1]+confusionmatrix[2,1])
    return recall1

def F1score(precision, recall):
    return 2/((1/precision)+(1/recall))

def precisiongeneral(confusionmatrix, objectindex):                            #object index is the object you are finding the precision for
    M1 = confusionmatrix[objectindex, objectindex]
    M2 = 0
    for i in range(confusionmatrix.shape[0]):
        M1 = M1 + confusionmatrix[objectindex, i]
    return M1/M2

def recallgeneral(confusionmatrix, objectindex):
    M1 = confusionmatrix[objectindex, objectindex]
    M2 = 0
    for i in range(confusionmatrix.shape[0]):
        M1 = M1 + confusionmatrix[i, objectindex]
    return M1/M2

def ReLUderivative(a):
    if a>0:
        return 1
    else:
       return 0

def softmaxderivative(softmax):               #with respect to a_i   #make a an numpyarray     #doublecheck this function
    derivatives = np.zeros(a.shape, a.shape)
    for i in range(a.shape):                   #i gives us which softmax and j gives which a in the derivative
        for j in range(a.shape):
            if i==j:
                derivatives [j, i] = softmax[i](1-softmax[i])
            else:
                derivatives[j, i] = -softmax[i]*softmax[j]

def sigmoidderivative(sigmoid):       #sigmoid is the value after a weighted sum uses the sigmoidactivation function 
    return sigmoid * (1 - sigmoid)
