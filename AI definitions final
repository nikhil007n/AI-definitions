import numpy as np
import matplotlib.pyplot as plt
import multiprocessing as mp
import math

def convolution(image, filter):
    empty = np.zeros((image.shape[0]-2, image.shape[0]-2))
    for j in range(26):
     for i in range(26):
         dot_1 = 0
         for l in range(filter.shape[0]):
             for k in range(filter.shape[1]):
                 dot_1 = dot_1 + image[i+l, j+k]*filter[l, k]
                 if k==filter.shape[1]-1 and l==filter.shape[0]-1:
                     empty[i,j] = dot_1
    return(empty)

def maxpool(convol, pool):
    empty = np.zeros((24, 24))         
    for j in range(0, 24):                                  #stride of 2 horizontally
     for i in range(0, 24):                                 #stride of 2 vertically
         #dot_1 = 0
         for l in range(pool):
             for k in range(pool):
                 pooling = np.zeros((2,2))
                 pooling[l,k] = convol[i+l, j+k]
                 
         empty[i, j] = np.max(pooling)        
          
                 #if dot_1 < convol[i+l, j+k]:
                     #dot_1 =  convol[i+l, j+k]*filter[l, k]
                 #if k==pool.shape[1]-1 and l==pool.shape[0]-1:
                     #empty[i,j] = dot_1
    return(empty)

def ReLUactivation(input):
     for i,v in enumerate(input):
         if v<0:
             input[i]=0
     return(input)

def softmax(x):                                          #Use np.float64 for input
  e_x = np.exp(x) 
  return e_x / e_x.sum(axis=0)

def sigmoid(x):
    return 1/(1+np.exp(-x))

def meansquaredcost(prob, hotoneempty):
 outputcost=0
 cost1 = 0
 outputcost = np.sum((prob-hotoneempty)**2)/10
 return outputcost

  #hotone representation of labels
def hotone(train_labels, no_of_final_outputs, j):
 hotoneempty = np.zeros(shape = (no_of_final_outputs))
 lol = int(train_labels[j, 0])
 hotoneempty[lol] = 1
  
 return hotoneempty

def forward1(input, weights, biases):   #make sure input, weights and biases are all numpy arrays
 neuron_no = len(input)
 output = weights @ input
 output1 = output + biases
 return output1

def img_diag_flip(input):
    input1 = np.asarray(input)
    return input1.T

def img_hori_flip(img_training):
    input1 = np.zeros((img_training.shape[0], img_training.shape[1]))
    for i in range(img_training.shape[0]):
        for j in range(img_training.shape[1]):
            input1[-i-1, j] = img_training[i, j]
    return input1

def img_vert_flip(img_training):
    input1 = np.zeros((img_training.shape[0], img_training.shape[1]))
    for i in range(img_training.shape[0]):
        for j in range(img_training.shape[1]):
            input1[i, -j-1] = img_training[i, j]
    return input1

def sigmoid(x):
    return 1 / (1 + math.exp(-x))

def CrossEntropy(yHat, y):
    if y == 1:
      return -log(yHat)
    else:
      return -log(1 - yHat)   
  
def confusionmatrix(TP, TN, FP, FN):
      return np.asarray([[TP, FP], [FN, TN]])

def accuracy(confusionmatrix):
     accuracy1 = (confusionmatrix[1,1] + confusionmatrix[2,2])/((confusionmatrix[1,1])+confusionmatrix[1,2]+confusionmatrix[2,1]+confusionmatrix[2,2])
     return accuracy1

def precision(confusionmatrix):
    precision1 = confusionmatrix[1,1]/(confusionmatrix[1,1]+confusionmatrix[1,2])
    return precision1

def recall(confusionmatrix):
    recall1 = confusionmatrix[1,1]/(confusionmatrix[1,1]+confusionmatrix[2,1])
    return recall1

def F1score(precision, recall):
    return 2/((1/precision)+(1/recall))

def precisiongeneral(confusionmatrix, objectindex):                            #object index is the object you are finding the precision for
    M1 = confusionmatrix[objectindex, objectindex]
    M2 = 0
    for i in range(confusionmatrix.shape[0]):
        M1 = M1 + confusionmatrix[objectindex, i]
    return M1/M2

def recallgeneral(confusionmatrix, objectindex):
    M1 = confusionmatrix[objectindex, objectindex]
    M2 = 0
    for i in range(confusionmatrix.shape[0]):
        M1 = M1 + confusionmatrix[i, objectindex]
    return M1/M2

def ReLUderivative(a):
    if a>0:
        return 1
    else:
       return 0

def softmaxderivative(softmax):               #with respect to a_i   #make a an numpyarray     #doublecheck this function
    derivatives = np.zeros((softmax.shape[0], softmax.shape[0]))
    for i in range(softmax.shape[0]):                   #i gives us which softmax and j gives which a in the derivative
        for j in range(softmax.shape[0]):
            if i==j:
                derivatives[j, i] = softmax[i]*(1-softmax[i])
            else:
                derivatives[j, i] = -softmax[i]*softmax[j]
    return derivatives

def sigmoidderivative(sigmoid):       #sigmoid is the value after a weighted sum uses the sigmoidactivation function 
    return sigmoid * (1 - sigmoid)

def ReLUderivative_of_weights(weights, outputs):         #weights is a list of weight matrices in order and outputs is a list of output vectors in order. Weight will be a normal list made of arrays
   for h in range(len(weights)):
     for i in range(weights[h].shape[0]):  
      if ReLUderivative(outputs[h][i]) < 0:
        for j in range(weights[h][0].shape[0]):
            weights[h][i,j] = 0
   return weights

def costMSEderivatives(softmax, hotone):                             #softmax is the softmax output vector. Make sure it's a normal list. Same with hotone.
    deriv = np.zeros(len(softmax))
    for i in range(len(softmax)):
        deriv[i]=2*(softmax[i]-hotone[i])
    return deriv


def running_backprop(weights, r, softmax, hotone, outputs):                            #Use weights from ReLUderivative_of_weights
    #for first weight matrix
    sum_of_cost_derivatives =  np.sum(costMSEderivatives(softmax, hotone))
    for i in range(weights[-1].shape[0]):
        for j in range(weights[-1].shape[1]):
            softmaxderiv_sum = np.sum(softmaxderivative(softmax)[i])
            weights[-1][i, j] = weights[-1][i, j] - r*sum_of_cost_derivatives*softmaxderiv_sum
    
            
    softmaxderiv_sum = np.sum(softmaxderivative(softmax))

    #for second weight matrix
    for i in range(weights[-2].shape[0]):
        for j in range(weights[-2].shape[1]):
            weightsum1 = 0
            for k in range(weights[-1].shape[0]):
               weightsum1 = weightsum1 + weights[-1][k, i]      #Need to doublecheck if this line is correct
            weights[-2][i, j] = weights[-2][i, j] - r*sum_of_cost_derivatives*softmaxderiv_sum*weightsum1

    
    sum_of_weights_minus1 = np.sum(weights[-1])
    sum_of_weights_minus2 = np.sum(weights[-2])
    for i in range(weights[-3].shape[0]):
        for j in range(weights[-3].shape[1]):
            weightsum2 = 0
            for k in range(weights[-2].shape[0]):
               weightsum2 = weightsum2 + weights[-2][k, i]      #Need to doublecheck if this line is correct
            weights[-3][i, j] = weights[-3][i,j] - r*sum_of_cost_derivatives*softmaxderiv_sum*weightsum2*sum_of_weights_minus1

    

    for i in range(weights[-4].shape[0]):
        for j in range(weights[-4].shape[1]):
            weightsum3 = 0
            for k in range(weights[-3].shape[0]):
               weightsum3 = weightsum3 + weights[-3][k, i]      #Need to doublecheck if this line is correct
            weights[-4][i, j] = weights[-4][i,j] - r*sum_of_cost_derivatives*softmaxderiv_sum*weightsum3*sum_of_weights_minus1*sum_of_weights_minus2
    
    return weights


#Lets try running a full model
trainingset = np.loadtxt("mnist_train.csv", delimiter=",")
testset = np.loadtxt("mnist_test.csv", delimiter=",")
training_imgdata = (np.asfarray(trainingset[:,1:])/(255))+0.1
test_imgdata = (np.asfarray(trainingset[:,1:])/255)+0.1
training_labels = np.asarray(trainingset[:,:1])
test_labels = np.asarray(testset[:,:1])
test_img = test_imgdata[0].reshape((int(math.sqrt(test_imgdata.shape[1])), int(math.sqrt(test_imgdata.shape[1]))))

weights1 = np.random.random((20, 576))
weights2 = np.random.random((20, 20))
weights3 = np.random.random((20, 20))
weights4 = np.random.random((10, 20))
weightsall = [weights1, weights2, weights3, weights4]

bias1 = np.random.random((20))
bias2 = np.random.random((20))
bias3 = np.random.random((20))
bias4 = np.random.random((10))

for i in range(training_labels.shape[0]):
 training_img = training_imgdata[i].reshape((int(math.sqrt(training_imgdata.shape[1])), int(math.sqrt(training_imgdata.shape[1]))))
 
 trainlabels=hotone(training_labels, 10, i)
 print(i)
 filter = np.asarray([[10, 20],[30, 40]])
 conv = convolution(training_img, filter)
 
 pooling = maxpool(conv, 2)
 
 input1 = pooling.flatten()
 input1 = np.asfarray(input1, dtype=np.float64)
 output1 = forward1(input1, weightsall[0], bias1)
 output2 = forward1(output1, weightsall[1], bias2)
 output3 = forward1(output2, weightsall[2], bias3)
 output4 = forward1(output3, weightsall[3], bias4)/10000000000
 outputall = [output1, output2, output3, output4]
 
 weightsReLU = ReLUderivative_of_weights(weightsall, outputall)
 
 probs = softmax(output4)
 
 cost = meansquaredcost(probs, trainlabels)
 
 weightsall = running_backprop(weightsReLU, 0.01, probs, trainlabels, outputall)
 

 #test
filter = np.asarray([[10, 20],[30, 40]])
conv = convolution(test_img, filter)
pooling = maxpool(conv, 2)
input1 = pooling.flatten()
input1 = np.asfarray(input1, dtype=np.float64)
output1 = forward1(input1, weightsall[0], bias1)
output2 = forward1(output1, weightsall[1], bias2)
output3 = forward1(output2, weightsall[2], bias3)
output4 = forward1(output3, weightsall[3], bias4)
outputall = [output1, output2, output3, output4]
probs = softmax(output4)
print(np.max(probs))
print(test_labels[0,0])

    
